# The Purpose Of a LLM Is What It Does

To all who're criticizing itself the mounting criticism of LLMs and who'd rather
like to emphasize these models can also be used for good:

POSIWID (aka The Purpose Of a System Is What It Does) is very much applicable
here, i.e. there is "no point in claiming that the purpose of a system is to do
what it constantly fails to do".[1]

For the moment (and I don't detect _any_ signs of this changing), LLMs
conceptually and the way they're handled technologically/politically, are
harmful, more than anything, regardless of other potential/actual use cases. In
a non-capitalist, solarpunk timeline this all might look very different, but
we're _absolutely not_ in that world. It's simply ignorant and impossible to
only consider LLM benefits anecdotally or abstractly, detached from their
implementation, their infrastructure required for training, the greed, the
abuse, the waste of resources (and resulting conflicts), the inflation,
disinformation, and tangible threats (with already real impacts) to climate,
energy, rights, democracy, society, life etc. These aren't hypotheticals — not
anymore!

A basic cost-benefit analysis:

In your eyes, are the benefits of LLMs worth these above costs? Could these
benefits & time savings have been achieved in other ways? Do you truly believe a
"democratization of skills" is achievable via the hyper-centralization of
resources, whilst actively harvesting and then removing the livelihood and
rights of entire demographics? You're feeling so very productive with your
copilot subscription, how about funding FLOSS projects instead and help building
sustainable/supportive communities? How about investing $500 billions into
education/science/arts?

Cybernetics was all about feedback loops, recursion, considering the effects of
a system and studying their influence on subsequent actions/iterations.
Technologists (incl. my younger self) have made the mistake/choice ignoring
tech's impact in the world for far too long. For this field to truly move
forward and become more holistic, empathetic and ethical, it _must_ stop
treating the above aspects as distracting inconvenient truths and start
addressing them head on, start considering secondary and tertiary effects of our
actions, and use those to guide us! Neglecting or actively denying their
importance and the more-than-fair criticism without ever being able to produce
equally important counter examples/reasons just make us look ignorant of the
larger picture... Same goes for education/educators in related disciplines!

Nothing about LLMs is inevitable per se. There's always a decision and for each
decision we have to ask who's behind it, for what purposes, who stands to
benefit and where do we stand with these. Sure, like any other tech, LLMs are
"just a tool", unbiased in theory, usable for both positive and negative
purposes. But, we've got to ask ourselves at which point a "tool" has attracted
& absorbed a primary purpose/form as a weapon (incl. usage in a class war), and
any other humanist aspects have become mere nice-to-have side effects, great for
greenwashing, and — for some — surfing the hype curve, while it lasts. We've got
to ask at which point LLMs currently are on this spectrum and in which direction
they're actively accelerating (are being accelerated)...

(Ps. Like many others, for many years I've been fascinated by, building and
using AI/ML techniques in many projects. I started losing interest shortly after
the introduction of GANs and the non-stop demand for exponentially increasing
hardware resources and obvious ways how this tech will be used in ever more
damaging ways... So my criticism isn't against AI as general field of research,
but about what is currently sold as AI and how it's being pushed onto us, for
reasons which actually have not much to do with AI itself, other than being a
powerful excuse/lever for enabling empire building efforts and possible societal
upheavals...)
